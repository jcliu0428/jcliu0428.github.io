<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jiachen Liu</title>

    <meta name="author" content="Jiachen Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiachen Liu
                </p>
                <p>
		I'm currently a machine learning engineer at Tiktok in San Jose. I received my PhD in Information Sciences and Technology from Penn State University in 2024, where I am fortunately to be advised by Dr. <a href="https://faculty.ist.psu.edu/suh972/">Sharon X. Huang</a>. Prior to that, I did my bachelor degree in Beihang University in China.
        My research interests and experience include 3D vision and generative AI. Specifically, I am interested in structured 3D reconstruction, scene understanding and their downstream applications, as well as generalizable 3D recontruction across various environments.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jiachenliu0428@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=NSeLtc0AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jiachen-liu-857b59168/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jcliu0428">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="my_images/Jiachenliu.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="my_images/Jiachenliu.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Selected Research</h2>
                <p>
                My primary interest lies in 3D vision and generative AI, including structured 3D reconstruction, scene understanding, generalizable 3D vision, scene layout generation and 3D-aware asset generation. (* indicates equal contribution.)
                </p>
              </td>
              </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            </tr>

            <tr>
            <td style="padding:4px;width:30%;vertical-align:middle">
            <img style="width:100%" src="my_images/zeroplane.png" alt="dise">
            </td>
            <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2506.02493">
                    <span class="papertitle">Towards In-the-wild 3D Plane Reconstruction from a Single Image
                    </span>
            </a>
            <br>
            <strong>Jiachen Liu*</strong>,
            Rui Yu*,
            Sili Chen,
            Sharon X. Huang,
            Hengkai Guo
            <br>
            <em>CVPR</em>, 2025 &nbsp <font color="red"><strong>(Highlight Presentation)</strong></font>
            <br>
            <a href="https://github.com/jcliu0428/ZeroPlane">code</a>
            /
            <a href="https://arxiv.org/abs/2506.02493">arXiv</a>
            <p></p>
            <p>
            We aim to propose the problem of <strong>in-the-wild, zero-shot</strong> 3D plane reconstruction. To this end, (1) We have constructed a large-scale benchmark dataset with high-quality dense planar annotations from multiple RGB-D datasets sampled across various indoor and outdoor environments. (2) We propose a Transformer-based framework on mixed-dataset training with a disentangled, classification-then-regression normal and offset learning paradigm to effectively
            handle the challenge raised in scale invariance across diverse indoor and outdoor scenes. Our model has demonstrated state-of-the-art planar reconstruction performance in terms of both accuracy and generalizability.
            </p>

            <tr>
            <td style="padding:4px;width:30%;vertical-align:middle">
            <img style="width:100%" src="my_images/floorplan_survey.png" alt="dise">
            </td>
            <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2504.09694">
                    <span class="papertitle">Computer-Aided Layout Generation for Building Design: A Review
                    </span>
            </a>
            <br>
            <strong>Jiachen Liu</strong>,
            Yuan Xue,
            Haomiao Ni,
            Rui Yu,
            Zihan Zhou,
            Sharon X. Huang;
            <br>
            <em>CVMJ</em>, 2025
            <br>
            <a href="https://arxiv.org/abs/2504.09694">arXiv</a>
            <p>
            We present a comprehensive survey on current methods on floorplan layout generation, scene synthesis and other miscellaneous layout generation topics. We also discuss valuable future perspectives to work on in this area.
            </p>
            </td>
            </tr>
 
            <tr>
            <td style="padding:4px;width:30%;vertical-align:middle">
            <img style="width:100%" src="my_images/monoplane_iros.png" alt="dise">
            </td>
            <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://github.com/thuzhaowang/MonoPlane">
                    <span class="papertitle">MonoPlane: Exploiting Monocular Geometric Cues for Generalizable 3D Plane Reconstruction
                    </span>
            </a>
            <br>
            Wang Zhao*,
            <strong>Jiachen Liu*</strong>,
            Sheng Zhang,
            Yishu Li,
            Sili Chen,
            Sharon X. Huang,
            Yong-Jin Liu,
            Hengkai Guo;
            <br>
            <em>IROS</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
            <br>
            <a href="https://github.com/thuzhaowang/MonoPlane">code (Coming soon)</a>
            /
            <a href="https://arxiv.org/abs/2411.01226">arXiv</a>
            <p></p>
            <p>
            Leverage pretrained geometric foundation models (depth and normal) for zero-shot monocular plane reconstruction.
            </p>
            </td>
            </tr>
 
            <tr>
            <td style="padding:4px;width:30%;vertical-align:middle">
            <img style="width:100%" src="my_images/neo_icra.png" alt="dise">
            </td>
            <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2309.13240">
                    <span class="papertitle">NeRF-Enhanced Outpainting for Faithful Field-of-View Extrapolation
                    </span>
            </a>
            <br>
            Rui Yu*,
            <strong>Jiachen Liu*</strong>,
            Zihan Zhou,
            Sharon X. Huang
            <br>
            <em>ICRA</em>, 2024
            <br>
            <a href="https://arxiv.org/abs/2309.13240">arXiv</a>
            <p></p>
            <p>
            We use NeRF to represent an indoor scene for novel view synthesis to augment the training set, then train a view extrapolation network on the densely sampled synthesized data to improve the model's view extrapolation capacity in a 3D coherent manner.
            </p>
            </td>
            </tr>

            <tr>
            <td style="padding:4px;width:30%;vertical-align:middle">
            <img style="width:100%" src="my_images/floorplan_eccv.png" alt="dise">
            </td>
            <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2207.13268">
                    <span class="papertitle">End-to-end Graph-constrained Vectorized Floorplan Generation with Panoptic Refinement
                    </span>
            </a>
            <br>
            <strong>Jiachen Liu*</strong>,
            Yuan Xue,
            Jose Duarte,
            Krishnendra Shekhawat,
            Zihan Zhou,
            Xiaolei Huang
            <br>
            <em>ECCV</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2207.13268">arXiv</a>
            <p></p>
            <p>
            We propose a Transformer-based two-stage framework to effecitvely generate vectorized interior floorplan layouts in an end-to-end manner.
            </p>
            </td>
            </tr>

            <tr>
            <td style="padding:4px;width:30%;vertical-align:middle">
            <img style="width:100%" src="my_images/planemvs.png" alt="dise">
            </td>
            <td style="padding:8px;width:70%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2203.12082">
                    <span class="papertitle">PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo
                    </span>
            </a>
            <br>
            <strong>Jiachen Liu</strong>,
            Pan Ji,
            Nitin Bansal,
            Changjiang Cai,
            Qingan Yan,
            Xiaolei Huang,
            Yi Xu
            <br>
            <em>CVPR</em>, 2022
            <br>
            <a href="https://github.com/oppo-us-research/PlaneMVS">code</a>
            /
            <a href="https://arxiv.org/abs/2203.12082">arXiv</a>
            <p></p>
            <p>
            We present a multi-view plane reconstruction framework based on the proposed slanted plane hypothesis, where we achieve state-of-the-art plane reconstruction as well as superior multi-view depth estimation across multiple benchmark datasets.
            </p>
            </td>
            </tr>
        
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr> 
            <td style="padding:16px;width:100%;vertical-align:middle">
            <h2>Academic Service</h2>
            <p>
            I serve as a regular reviewer for top-tier CV/ML conferences, such as CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, etc.
            </p>
            </td>
            </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    &copy; Jiachen Liu | Last updated: June 2025</a>
                </p>
                <p style="text-align:center;font-size:small;">
                    Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>

  </body>
</html>
